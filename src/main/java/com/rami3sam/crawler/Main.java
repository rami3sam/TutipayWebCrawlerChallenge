/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package com.rami3sam.crawler;

import java.util.ArrayList;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

public class Main {
    //static String SEED_URL = "http://localhost:8000/dir/test.html";
    static String SEED_URL = "http://monzo.com";

    static String DOMAIN = "opera.com";
    static int CRAWL_LIMIT = 100;
    static volatile HashSet crawledURLs = new HashSet();
    static volatile ArrayList<String> newURLs = new ArrayList<>();

    public static void main(String[] args) throws InterruptedException {
        newURLs.add(SEED_URL);

        ExecutorService executor = Executors.newFixedThreadPool(5);//creating a pool of 5 threads
        for (int i = 0; i < CRAWL_LIMIT; i++) {
            String currentURL;

            // if there is no new urls in netURLS wait for a new one
            while (newURLs.size() <= i ) {}

            currentURL = newURLs.get(i);


            if (!crawledURLs.contains(currentURL)) {
                Main.crawledURLs.add(currentURL);
                CrawlerRunnable worker = new CrawlerRunnable(currentURL);
                executor.execute(worker);//calling execute method of ExecutorService
            }

        }
        executor.shutdown();
        while (!executor.isTerminated()) {
        }

        System.out.println("************\n");

    }
}
