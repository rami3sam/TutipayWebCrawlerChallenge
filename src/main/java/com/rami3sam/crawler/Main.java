/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package com.rami3sam.crawler;

import java.io.FileWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

public class Main {
    //static String SEED_URL = "http://localhost:8000/dir/test.html";
    static String SEED_URL = "http://monzo.com";

    static String DOMAIN = "opera.com";
    static int CRAWL_LIMIT = 100;
    static volatile HashSet crawledURLs = new HashSet();
    static volatile ArrayList<String> newURLs = new ArrayList<>();

    static FileWriter outputFileWriter = null;

    public static void main(String[] args) throws IOException {
        newURLs.add(SEED_URL);

        try {
            outputFileWriter = new FileWriter("links.txt");
        } catch (IOException e) {
            System.err.println("Couldn't open links.txt for output writing");
        }

        ExecutorService executor = Executors.newFixedThreadPool(35);//creating a pool of 5 threads
        for (int i = 0; true; i++) {
            String currentURL;

            if(i > 0 && executor.isTerminated()){
                break;
            }
            // if there is no new urls in netURLS wait for a new one
            while (newURLs.size() <= i ) {
            }


            currentURL = newURLs.get(i);


            if (!crawledURLs.contains(currentURL)) {
                Main.crawledURLs.add(currentURL);
                CrawlerRunnable worker = new CrawlerRunnable(currentURL);
                executor.execute(worker);//calling execute method of ExecutorService
            }

        }
        executor.shutdown();
        while (!executor.isTerminated()) {
        }


        outputFileWriter.close();

        System.out.println("************\n");

    }
}
