/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package com.rami3sam.crawler;

import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;

import java.net.MalformedURLException;
import java.net.URI;
import java.net.URISyntaxException;
import java.net.URL;
import java.util.ArrayList;
import java.util.regex.Pattern;

public class Util {

    static Pattern urlWithSchemeRegexp = Pattern.compile("[a-z+-.]+:.*", Pattern.CASE_INSENSITIVE);
    public static ArrayList<String> getPageLinks(String pageURL) {
        try {

            URI pageURLObject = new URI(pageURL);
            Document document = Jsoup.connect(pageURL).get();
            Elements anchorTags = document.getElementsByTag("a");

            ArrayList<String> links = new ArrayList<>();

            for (Element anchorTag : anchorTags) {
                String scrapedURL = anchorTag.attr("href");
                scrapedURL = rebaseIfRelativeUrl(pageURLObject, scrapedURL);

                // check if the URL is well-formed if it is, proceed to add it to the list else skip it
                URL url;
                try {
                    url = new URL(scrapedURL);
                } catch (MalformedURLException e) {
                    //e.printStackTrace();
                    continue;
                }

                //Strip the fragment from the url to get rid of duplicates
                String urlString = stripUrlFragmentAndNormalize(url);

                // only add http and https links since only them can be crawled by this crawler
                if (checkForCrawlableURLScheme(url)) {
                    links.add(urlString);
                }
            }

            return links;

            //handle the exception of the function being given a malformed url
        } catch (URISyntaxException e) {
            //e.printStackTrace(System.err);
            return null;
            // handle the exception of the function not being able to start the connection
        } catch (Exception e) {
            //e.printStackTrace(System.err);
            return null;
        }
    }

    private static boolean checkForCrawlableURLScheme(URL url) {
        String protocol = url.getProtocol();
        return protocol.equalsIgnoreCase("http") || protocol.equalsIgnoreCase("https");
    }

    private static String stripUrlFragmentAndNormalize(URL url) {
        String retuUrl = url.getProtocol().toLowerCase() + "://" + url.getAuthority().toLowerCase() + url.getFile();

        // to get red of duplicates https://rami.com/page and https://rami.com/page/
        if (retuUrl.endsWith("/")){
            retuUrl = retuUrl.substring(0,retuUrl.length() - 1);
        }
        return retuUrl;
    }

    private static String rebaseIfRelativeUrl(URI baseURL, String relativeURL) {
        if (relativeURL.startsWith("//")){
            relativeURL = baseURL.getScheme() +  ":"+ relativeURL;
        }

        if (!Util.urlWithSchemeRegexp.matcher(relativeURL).find()) {
            // in case the url is a relative url add the current pages host as a prefix

            // if it is just a fragment or query add current pages path
            if (relativeURL.startsWith("#") || relativeURL.startsWith("?")) {
                relativeURL = baseURL.getPath() + relativeURL;
            } else if (!relativeURL.startsWith("/")){
                //this is for urls relative to the parent directory
                URI parent = baseURL.getPath().endsWith("/") ? baseURL.resolve("..") : baseURL.resolve(".");
                relativeURL = parent.getPath() + relativeURL;
            }
            relativeURL = baseURL.getScheme() + "://" + baseURL.getAuthority() + relativeURL;

        }
        return relativeURL;
    }

    public static boolean isOneSubdomainOfTheOther(String a, String b) {

        try {
            URL first = new URL(a);
            String firstHost = first.getHost();
            firstHost = firstHost.startsWith("www.") ? firstHost.substring(4) : firstHost;

            URL second = new URL(b);
            String secondHost = second.getHost();
            secondHost = secondHost.startsWith("www.") ? secondHost.substring(4) : secondHost;

            /*
             Test if one is a substring of the other
             */
            if (firstHost.contains(secondHost) || secondHost.contains(firstHost)) {

                String[] firstPieces = firstHost.split("\\.");
                String[] secondPieces = secondHost.split("\\.");

                String[] longerHost = {""};
                String[] shorterHost = {""};

                if (firstPieces.length >= secondPieces.length) {
                    longerHost = firstPieces;
                    shorterHost = secondPieces;
                } else {
                    longerHost = secondPieces;
                    shorterHost = firstPieces;
                }
                //int longLength = longURL.length;
                int minLength = shorterHost.length;
                int i = 1;

                /*
                 Compare from the tail of both host and work backwards
                 */
                while (minLength > 0) {
                    String tail1 = longerHost[longerHost.length - i];
                    String tail2 = shorterHost[shorterHost.length - i];

                    if (tail1.equalsIgnoreCase(tail2)) {
                        //move up one place to the left
                        minLength--;
                    } else {
                        //domains do not match
                        return false;
                    }
                    i++;
                }
                if (minLength == 0) //shorter host exhausted. Is a sub domain
                    return true;
            }
        } catch (MalformedURLException ex) {
            ex.printStackTrace();
        }
        return false;
    }
}
